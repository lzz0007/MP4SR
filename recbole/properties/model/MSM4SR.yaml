n_layers: 2
n_heads: 2
hidden_size: 64
inner_size: 256
hidden_dropout_prob: 0.2
attn_dropout_prob: 0.2
hidden_act: 'gelu'
layer_norm_eps: 1e-12
initializer_range: 0.02
loss_type: 'CE'


train_stage: pretrain  # pretrain / finetune
num_imgtokens: 15 # number of image word tokens
nip_weight: 1.0 # weight of NIP
lambda: 0.01 # weight to balance between NIP and CMCL
adaptor_dropout_prob: 0.2
adaptor_layers: [768,64] # MoE layer dimension
temperature: 0.07 # temperature for contrastive losses
n_exps: 8 # number of MoE experts
mflag: False # whether to disable the sequence mixup
